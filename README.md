# NLP Headline–Body Sentiment Gap (GVFC)

This repo is a **reproducible NLP research pipeline** that quantifies how news **headlines** differ from article **body text** in sentiment, then studies how that *sentiment gap* varies across **topics/annotations** and (optionally) across **publisher ideology/bias**.

The pipeline is designed to work in two modes:
- **Summary mode (default, fully offline):** uses dataset-provided article summaries as the “body”.
- **Full-text mode (optional):** uses scraped full article text (if you run the scraper and have `data/full_text/*.txt`).

## Research goals (expanded / improved)

This repo aims to be more than “headline vs body sentiment”; it is a **framing measurement system**:

- **End-to-end dataset construction**: unify multiple GVFC annotation sources into a single, validated analysis table with stable schemas and versioned outputs.
- **Headline vs body framing gap**: compute sentiment for headlines and bodies using a transformer model, with robust long-text chunking and reproducible scoring.
- **Robust inference**: quantify uncertainty (bootstrap confidence intervals), effect sizes, and non-parametric tests; track sensitivity to modeling and preprocessing choices.
- **Structured subgroup analysis**: measure how gaps differ by annotation dimensions (e.g., `Q2 Focus`, `Q3 Theme1/Theme2`) and by outlet.
- **Outlet / ideology analysis (optional)**: extract publisher domains, merge an external bias-rating table if present, and visualize outlet-level patterns.
- **Reproducibility & robustness**: deterministic outputs, caching, data validation, explicit data contracts, and clear pipeline entrypoints.

## Project structure

```text
data/
  GVFC_extension_multimodal.csv               # source dataset (master)
  GVFC_headlines_and_annotations.xlsx         # auxiliary headline-only annotations
  clean_gvfc_sentiment.csv                    # derived (legacy from notebooks)
  clean_gvfc_sentiment_v2.csv                 # derived (legacy from notebooks)
  full_text/                                 # optional: scraped full article texts {id}.txt
  raw_html/                                  # optional: cached HTML {id}.html
  external/                                  # optional: external annotations (e.g. bias table)
  derived/                                   # generated by pipeline (canonical outputs)

src/nlp_headline_body_sentiment/
  pipeline.py                                # CLI + orchestration
  datasets.py                                # dataset builder + schema standardization
  cleaning.py                                # text cleaning utilities
  sentiment.py                               # transformer sentiment scoring + chunking + caching
  analysis.py                                # gap metrics + stats + robustness checks
  bias.py                                    # domain normalization + optional bias merges
  viz.py                                     # figure generation

notebooks/
  00_scrape.ipynb
  01_explore.ipynb
  02_preprocess_clean.ipynb
  03_sentiment_model.ipynb
  04_gap_and_stats.ipynb
  05_visualize.ipynb
  06_outlet_bias.ipynb
```

## Quickstart

### 1) Create environment

Use Python 3.10+.

```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
pip install -e .
```

### 2) Build the canonical dataset (summary mode)

```bash
python -m nlp_headline_body_sentiment run-all --mode summary
```

Outputs are written under `data/derived/` and figures under `reports/figures/`.

This command also writes a lightweight narrative report:
- `reports/report_summary.md`

### 3) Optional: run full-text scraping

Scraping is brittle by nature (paywalls, dead links, rate limits). If you choose to scrape:

```bash
python -m nlp_headline_body_sentiment scrape --limit 50
python -m nlp_headline_body_sentiment run-all --mode fulltext
```

## Robustness (recommended)

To compute a fast, offline robustness baseline alongside the transformer model:

```bash
python -m nlp_headline_body_sentiment.pipeline run-all --mode summary --vader
```

This writes `data/derived/model_comparison_summary.csv` including transformer vs VADER agreement metrics.

## Data ethics & reproducibility

- **Scraping**: ensure you have the right to scrape and store content for research; respect robots/terms; rate limit.
- **External bias tables**: licensing and provenance matter. This repo treats bias as *optional* and does not assume it is redistributable.
- **Model dependence**: sentiment is model-dependent; use this repo to study robustness across models/settings.


