{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1299, 4)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "df = pd.read_csv(\"../data/full_articles_clean.csv\")\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import html\n",
    "\n",
    "# ──────────────────────────────────────────────────────────\n",
    "# Regex pre‑compilation (faster when called thousands of times)\n",
    "URL_RE      = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "EMAIL_RE    = re.compile(r'\\b\\S+@\\S+\\b')\n",
    "HTML_ENT_RE = re.compile(r'&[a-z]+;')\n",
    "WS_RE       = re.compile(r'\\s+')\n",
    "\n",
    "def light_clean(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Minimal text cleaning for sentiment analysis.\n",
    "    • lowercases\n",
    "    • strips URLs, e‑mail addresses, HTML entities (&amp;, &quot;, …)\n",
    "    • converts common HTML entities to utf‑8 (e.g. &amp; -> &)\n",
    "    • normalises fancy quotes -> straight quotes\n",
    "    • collapses repeated whitespace\n",
    "    Leaves punctuation, stop‑words, and emojis intact because\n",
    "    they often carry sentiment signal.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # 1) lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2) remove URLs & e‑mails\n",
    "    text = URL_RE.sub(\" \", text)\n",
    "    text = EMAIL_RE.sub(\" \", text)\n",
    "\n",
    "    # 3) unescape & drop remaining HTML entities\n",
    "    text = html.unescape(text)           # &amp; -> &\n",
    "    text = HTML_ENT_RE.sub(\" \", text)\n",
    "\n",
    "    # 4) replace “smart quotes” with \"\n",
    "    text = text.replace(\"“\", '\"').replace(\"”\", '\"') \\\n",
    "               .replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
    "\n",
    "    # 5) collapse whitespace\n",
    "    text = WS_RE.sub(\" \", text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"headline_clean\"] = df[\"headline\"].apply(light_clean)\n",
    "df[\"body_clean\"]     = df[\"body_text\"].apply(light_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "/Users/araj/Documents/Code/Python Files/NLP Projects/NLP_headline_body_sentiment/venv/lib/python3.11/site-packages/transformers/pipelines/text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model     = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "\n",
    "device      = 0 if torch.cuda.is_available() else -1          # -1 → CPU\n",
    "sent_pipe   = pipeline(\"sentiment-analysis\",\n",
    "                       model=model,\n",
    "                       tokenizer=tokenizer,\n",
    "                       device=device,\n",
    "                       return_all_scores=True,\n",
    "                       truncation=True,\n",
    "                       max_length=512,\n",
    "                       batch_size=32)                         # tune for RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL2SCORE = {\"positive\": 1, \"neutral\": 0, \"negative\": -1}\n",
    "\n",
    "def weighted_score(result):\n",
    "    \"\"\"Turn list of {'label','score'} dicts into a single number.\"\"\"\n",
    "    return sum(d[\"score\"] * LABEL2SCORE[d[\"label\"].lower()] for d in result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_by_tokens(text, tokenizer, tokens_per_chunk=450):\n",
    "    \"\"\"\n",
    "    Split a long string into pieces, each ≤ tokens_per_chunk,\n",
    "    **without** needing sentence tokenisation.\n",
    "    \"\"\"\n",
    "    # Encode once to avoid repeated tokenisation\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    for i in range(0, len(tokens), tokens_per_chunk):\n",
    "        chunk_ids = tokens[i : i + tokens_per_chunk]\n",
    "        yield tokenizer.decode(chunk_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test headline: cremated remains of las vegas mass shooter to be kept in safe deposit box, brother says\n",
      "Type of result: <class 'list'>\n",
      "Result structure: [[{'label': 'negative', 'score': 0.1584518402814865}, {'label': 'neutral', 'score': 0.8270642161369324}, {'label': 'positive', 'score': 0.014483900740742683}]]\n"
     ]
    }
   ],
   "source": [
    "# Add this cell to debug the output structure\n",
    "test_headline = df[\"headline_clean\"].iloc[0]\n",
    "result = sent_pipe(test_headline)\n",
    "print(f\"Test headline: {test_headline}\")\n",
    "print(f\"Type of result: {type(result)}\")\n",
    "print(f\"Result structure: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1299/1299 [01:01<00:00, 21.06it/s]\n",
      "100%|██████████| 1299/1299 [18:12<00:00,  1.19it/s] \n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "# 1️⃣  Headlines – text is short, single pass\n",
    "df[\"sent_head\"] = df[\"headline_clean\"].progress_apply(\n",
    "    lambda x: weighted_score(sent_pipe(x)[0])\n",
    ")\n",
    "\n",
    "# 2️⃣  Bodies – may be long, so chunk then average\n",
    "def body_sentiment(text):\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return None\n",
    "    chunks  = list(chunk_by_tokens(text, tokenizer))\n",
    "    results = sent_pipe(chunks)               # returns list of lists\n",
    "    scores  = [weighted_score(r) for r in results]\n",
    "    return sum(scores) / len(scores)\n",
    "\n",
    "df[\"sent_body\"] = df[\"body_clean\"].progress_apply(body_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         sent_head    sent_body\n",
      "count  1299.000000  1299.000000\n",
      "mean     -0.402119    -0.258562\n",
      "std       0.340166     0.410197\n",
      "min      -0.928732    -0.932480\n",
      "25%      -0.689916    -0.616164\n",
      "50%      -0.450826    -0.259065\n",
      "75%      -0.131545     0.024327\n",
      "max       0.829489     0.967569\n",
      "                                       headline_text  sent_head  sent_body\n",
      "0  Cremated remains of Las Vegas mass shooter to ...  -0.143968  -0.324479\n",
      "1  Florida shooter a troubled loner with white su...  -0.777282  -0.812513\n",
      "2  Vernon Hills teen accused of wearing white sup...  -0.472589  -0.116213\n"
     ]
    }
   ],
   "source": [
    "print(df[[\"sent_head\", \"sent_body\"]].describe())\n",
    "print(df.head(3)[[\"headline_text\", \"sent_head\", \"sent_body\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/full_articles_with_sentiment.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
